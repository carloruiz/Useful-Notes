{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Many models, K-NN and linear models particularly, benefit substantially from scaling (notable exceptions are NN, and Tree based models). We cover a few methods readily available in Scikit-learn and other packages. (i denotes row, j column)  \n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, MaxAbsScaler`  \n",
    "\n",
    "**&#9755; StandardScaler:**  \n",
    "\n",
    "$\\large{x_{ij}' = \\frac{x_{ij} - \\mu_j}{\\sigma_j}} \\quad \\forall i,j$\n",
    "\n",
    "all scaled features now have zero mean and std of 1. Note that scaled data is not bounded since it is only scaled by the\n",
    "standard deviation. An outlier before the transformation will still be an outlier after transformation. StandardScaler is sensitive to outliers as they skew the mean and standard deviation. \n",
    "\n",
    "\n",
    "**&#9755; MinMaxScaler**  \n",
    "\n",
    "$\\large{x_{i,j}' = \\frac{x_{ij} - min(x_j)}{max(x_j) - min(x_j)}}$  \n",
    "\n",
    "maps the minimum of each column to 0 and max to 1. It is easy to see in the transformation that MinMaxScaler is very sensitive to outliers  since it will cluster points in one area of the positive unit quadrant. Useful when data has clearly defined boundaries i.e. greyscale image.  \n",
    "\n",
    "\n",
    "**&#9755; MaxAbsScaler**  \n",
    "Similar to MinMaxScaler except min and max are measured in absolute value. Useful for sparse data. \n",
    "\n",
    "**&#9755; RobustScaler**  \n",
    "Some robust statistics stuff (read interquartile range (IQR) and the median absolute deviation (MAD)). Similar to\n",
    "StandardScaler, except robust to outliers.  \n",
    "\n",
    "**&#9755; Normalizer**  \n",
    "Projects unto the L1 or L0 unit ball (i.e. makes sure vectors have length 1 either in euclidean measure or L1 measure). Can't think of use cases atm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls\n",
    "\n",
    "**&#9841; Scaling Sparse data:**  \n",
    "Do not center sparse data (i.e. apply zero mean, unit variance or MinMaxScaler) since this will make the matrix not sparse anymore and blow up RAM and CPU. Scale by a constant factor since constant times zero is zero, preserving sparsity. Use MaxAbsScaler for sparse data.  \n",
    "\n",
    "**&#9841; Including test set in scaler's `fit()`**  \n",
    "Including the test set in `fit()` will lead to artificially higher accuracy scores since we are parameterizing our scaler based on our test set. In deployment, we obviously can't parameterize our scalers on unseen data. \n",
    "\n",
    "**&#9841; Calling `fit()` on training and test set separately:**  \n",
    "Never call `fit()` on the test set separately since it will change the relationship of test points to training points. Only call fit on training set. Then call transform on both training and test sets. \n",
    "\n",
    "**&#9841; Not using pipelines in cross validation:**  \n",
    "When you perform cross validation on the scaled training set, the validation fold is scaled in the same way as the training fold, leading to pitfall 2. Using pipelines solves this pitfall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"files/images/scaling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# this is wrapper for Pipeline below\n",
    "pipe = make_pipeline(StandardScaler(), Ridge())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)\n",
    "\n",
    "# this gives flexibility in naming the steps. Useful when tuning parameters.\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline(((\"scaler\", StandardScaler()),\n",
    "                 (\"regressor\", KNeighborsRegressor)))\n",
    "\n",
    "# cross validation with pipeline\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "scores = cross_val_score(knn_pipe, X_train, y_train, cv=10)\n",
    "np.mean(scores), np.std(scores)\n",
    "\n",
    "\n",
    "# pipeline and GridSearchCV. Use modified names ([step_name]__[parameter]) \n",
    "# in GridSearch param_grid\n",
    "knn_pipe = make_pipeline(('scale', StandardScaler()), ('model', KNeighborsRegressor())\n",
    "param_grid = {'model__n_neighbors': range(1, 10)}\n",
    "grid = GridSearchCV(knn_pipe, param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Distributions \n",
    "\n",
    "Linear Models may perform better when features are normally distributed. Scaling data does not change the distribution of the points. There are several transformations available that do just this. Most common tranformations are power transformations, particularly the Box Cox Transformation.  \n",
    "\n",
    "**&#9755; Box-Cox transformation:**  \n",
    "\n",
    "$bc_{\\lambda}(x) = \\cases{\\frac{x^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0\\cr log(x) & \\text{if } \\lambda = 0 }$  \n",
    "\n",
    "only applicable for positive x!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearn 0.20-dev\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='box-cox') \n",
    "# soon: method='Yeo-Johnson'\n",
    "pt.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data  \n",
    "\n",
    "We can one-hot encode a feature's categorical values (e.g. $\\{'green', 'blue', 'yellow'\\}$) using pandas. We can encode all k values, introducing redundancies, or k-1 to have more accurate interpretation of coefficients. We can also one hot encode using sklearn but this method requires categorical values to be in integer format.  \n",
    "  \n",
    "We face a problem when categorical feature values are of high cardiniality (50, 100, etc). We must find a way to compress the values into fewer than k features. This solution is often specific to the dataset. \n",
    "\n",
    "## Pitfalls\n",
    "\n",
    "**&#9841; Categorical Values in test but not training set:**  \n",
    "Make sure to one hot encode all possible values of the feature, not just those in the test and traning sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'salary': [103, 89, 142, 54, 63, 219],\n",
    "                   'boro': ['Manhattan', 'Queens', 'Manhattan',\n",
    "                            'Brooklyn', 'Brooklyn', 'Bronx']})\n",
    "\n",
    "# note that there are more categories in the definition of the \n",
    "# column than those seen in the column\n",
    "df['boro'] = pd.Categorical(\n",
    "    df.boro, categories=['Manhattan', 'Queens', 'Brooklyn', 'Bronx', 'Staten Island'])\n",
    "pd.get_dummies(df, columns=['boro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature interactions  \n",
    "\n",
    "Linear models particularly benefit from feature interactions. Feature transformations allow models to fit non linear boundaries or curves to the data. However, these transformations blow up the feature space. Using kernel transformations allow for the power of the transformations without a significant increase in CPU or memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation  \n",
    "\n",
    "Imputation refers to dealing with missing values. We can always drop columns with NaN values but we would lose all the information given by non NaN values. We could also drop the observations that contain NaN values if they are few. There are 4 types of imputation methods that try to extrapolate missing values: Mean/median, kNN, regression, probabilistic. A general good practice with imputation is to create a dummy indicating if the value was NaN in addition to extrapolation of the value. Check out fancyimpute library (note that fancy impute does not implement fit/transform paradigm, thus does not work with pipelines...information leak!)\n",
    "\n",
    "**&#9755; Mean Imputation:**  \n",
    "Mean imputation fills in NaN values with the mean of the given feature. This, of course, only works for non binary data. Mean imputation can be acceptable if missing values are few. Else, this method can destroy the data distribution, hiding useful structural relationships between the input and target data.  \n",
    "\n",
    "**&#9755; kNN Imputation:**  \n",
    "kNN imputation works by taking k nearest neighbors of the observation with the missing value, and replaces NaN with the average value of that feature among the k neighbors. Note that kNN imputation only works if the features used to compute distance are not NaN, since Euclidiean distance is undefined for NaN values. So points with NaN values are thown away when computing  k neighbors.  \n",
    "\n",
    "**&#9755; Model Driven Imputation:**  \n",
    "Train a model on the non missing features to predict missing features (kNN imputation is arguably Model driven imputation but there are some slight differences). A popular model to use is random forests (see code below). \n",
    "\n",
    "**&#9755; (MICE) Multiple Imputation by Chained Equations:**  \n",
    "Not sure yet how it works but this is a very popular imputation method. Not in sklearn, but is in fancyimpute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-dc41b017f0c6>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-dc41b017f0c6>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    <img src=\"files/images/imputation_methods.png\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# mean imputation\n",
    "from sklean.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(strategy='mean').fit(X_train)\n",
    "imp.transform(X_train)\n",
    "\n",
    "\n",
    "# kNN imputation: very inefficient didactic implementation\n",
    "# use library!\n",
    "distances = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "for i, x1 in enumerate(X_train):\n",
    "    for j, x2 in enumerate(X_train):\n",
    "        dist = (x1 - x2) ** 2\n",
    "        nan_mask = np.isnan(dist)\n",
    "        distances[i, j] = dist[~nan_mask].mean() * X_train.shape[1]\n",
    "neighbors = np.argsort(distances, axis=1)[:, 1:]\n",
    "n_neighbors = 3\n",
    "X_train_knn = X_train.copy()\n",
    "for feature in range(X_train.shape[1]):\n",
    "    has_missing_value = np.isnan(X_train[:, feature])\n",
    "    for row in np.where(has_missing_value)[0]:\n",
    "        neighbor_features = X_train[neighbors[row], feature]\n",
    "        non_nan_neighbors = neighbor_features[~np.isnan(neighbor_features)]\n",
    "        X_train_knn[row, feature] = non_nan_neighbors[:n_neighbors].mean()\n",
    "    \n",
    "    \n",
    "# Model Driven Imputation with random forests.     \n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "X_imputed = X_train.copy()\n",
    "for i in range(10):\n",
    "    last = X_imputed.copy()\n",
    "    for feature in range(X_train.shape[1]):\n",
    "        inds_not_f = np.arange(X_train.shape[1])\n",
    "        inds_not_f = inds_not_f[inds_not_f != feature]\n",
    "        f_missing = np.isnan(X_train[:, feature])\n",
    "        rf.fit(X_imputed[~f_missing][:, inds_not_f], X_train[~f_missing, feature])\n",
    "        X_imputed[f_missing, feature] = rf.predict(\n",
    "            X_imputed[f_missing][:, inds_not_f])\n",
    "    if (np.linalg.norm(last - X_imputed)) < .5:\n",
    "        break\n",
    "scores = cross_val_score(logreg, X_imputed, y_train, cv=10)\n",
    "np.mean(scores)\n",
    "\n",
    "\n",
    "# MICE with fancyimpute\n",
    "import fancyimpute\n",
    "\n",
    "mice = fancyimpute.MICE(verbose=0)\n",
    "X_train_fancy_mice = mice.complete(X_train)\n",
    "scores = cross_val_score(logreg, X_train_fancy_mice, y_train, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/imputation_methods.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection  \n",
    "\n",
    "It is often beneficial to reduce the feature space. It leads to faster predictions and faster traning times. Less data to gather when model is in production. Less storage for model and dataset and more importantly, leads to more interpretable models. There are supervised and unsupervised feature selection methods.  \n",
    "\n",
    "**&#9755; Covariance based:**  \n",
    "When two features are highly correlated, we can remove 1 of them without substantially affecting model predictions (probably..it could be the small difference in variation of these features was really important for predictions. We can test this easily though.) Show code for sorting correlation matrix heatmap.\n",
    "\n",
    "**&#9755; PCA:**  \n",
    "PCA maps the data to a linear subpsace. Only reduces feature space for training and predictions, not for data gathering. It makes model less interpretable BUT it can lead to very useful visualizations in 2D and 3D and it can speed up training time considerably. It can remove useful information. \n",
    "\n",
    "**&#9755; Univariate Statistics:**  \n",
    "We can build a linear regression model on single feature and the target and measure F and p values for the coefficient. These values let us know if feature is important for prediction. However, linear regression assumes linear relationship between input and target, which may be a poor assumption. Furthermore, the linear assumption decreases the importance of binary features.  \n",
    "\n",
    "Mutual information relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances. MI Can deal with binary features (must specify which columns are binary). This is a good choice when assuming non linear interaction between feature and target. However, much more computationally intensive than f regression.  \n",
    "\n",
    "**&#9755; Model Based feature selection:**  \n",
    "Train a model and check coefficients (in the case of linear models) or splits (in the case of tree based models). Discard features with coefficient close to zero or without/non informative splits. This is usually a good choice but computationally expensive. \n",
    "\n",
    "**&#9755; Iterative Model based feature selection:**  \n",
    "Removing many features at once may change the model more than we indend to. A better way to iteratively remove a single feature, each time retraining the model and selecting the least important. We can do this with RFE (Recursive Feature Elimination) in Sklearn. We can also iterate the opposite way, Sequential Feature Selection from library mlextend. \n",
    "\n",
    "\n",
    "## Pitfalls  \n",
    "\n",
    "**&#9841; Univariate statistics do not account for correlation:**  \n",
    "Univariate statistics give highly correlated features the same importance. Model driven feature selection accounts for correlation and can thus give much higher importance to some features while removing importance from others (which is which is usually random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get F and p values so we can visualize and drop features\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "f_values, p_values = f_regression(X, y)\n",
    "\n",
    "\n",
    "# or we can use SelectKBest to automatically drop features below a threshold. \n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFpr\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "\n",
    "select = SelectKBest(k=2, score_func=f_regression)\n",
    "select.fit(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(select.transform(X_train).shape)\n",
    "\n",
    "# SelectKBest with pipelines\n",
    "make_pipeline(StandardScaler(), SelectKBest(k=2, score_func=f_regression), RidgeCV())\n",
    "\n",
    "# Mutual information\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "scores = mutual_info_regression(X_train, y_train, discrete_features=[3])\n",
    "\n",
    "# Model Driven feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "select_lassocv = SelectFromModel(LassoCV(), threshold=1e-5)\n",
    "select_lassocv.fit(X_train, y_train)\n",
    "print(select_lassocv.transform(X_train).shape)\n",
    "\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# create ranking among all features by selecting only one\n",
    "rfe = RFE(LinearRegression(), n_features_to_select=1)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "rfe.ranking_\n",
    "\n",
    "# RFE that automatically selects best number of features to keep\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfe = RFECV(LinearRegression(), cv=10)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "print(rfe.support_)\n",
    "print(boston.feature_names[rfe.support_])\n",
    "\n",
    "# Sequential Feature Selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "sfs = SequentialFeatureSelector(LinearRegression(), forward=False, k_features=7)\n",
    "sfs.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dimensionality Reduction  \n",
    "\n",
    "Transforming data using unsupervised learning can have many motivations. The most common motivations are visualization, compressing the data, and finding a representation that is more informative for further processing.\n",
    "\n",
    "## PCA\n",
    "\n",
    "On a high level, principal component analysis iteratively does the following: \n",
    "\n",
    "Finds the direction, orthogonal to all previously established directions, along which the data points vary the most (highest variance). Describe this direction with a unit vector. Continue until we have n directions. Build an n-dimensional linear map with these n vectors.\n",
    "\n",
    "These n vectors, representing n directions are called principal components. For n dimensional data, an n-dimensional PCA is just a rotation (re-basing of the vector space) in which the dimensions are sorted in order of decreasing variance. \n",
    "\n",
    "We usually scale the the data to have unit standard deviationg before applying PCA (PCA normalizes to 0 mean under the covers. Remember huge improvements in PCA benefits after applying StandardScaler on data in hw4 problem). It is important to note that PCA is an unsupervised method, and does not use any class information when finding the direction of maxiumum variance. It simply looks at the correlations in the data. PCA can substantially reduce the input data's dimensionality, greatly speeding up model training. PCA can also lead to increased model accuracy since dimensionality reduction decreased overfitting, which can in turn result in a better model. \n",
    "\n",
    "TODO talk about PCA equation  \n",
    "\n",
    "TODO Whitening\n",
    "\n",
    "TODO kernel PCA\n",
    "\n",
    "## Pitfalls  \n",
    "\n",
    "**&#9841; Uneven class distributions**  \n",
    "When our dataset has skewed distributions (or outliers), PCA will give a lot of weight to the classes with many observations. We need to randomly throw out observations of commonly occuring classes so most classes have similar occurence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"files/images/pca-intuition.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_train_scaled = StandardScaler(X_train)\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold learninf: t-SNE\n",
    "\n",
    "While PCA is often a good first approach for transforming your data so that you might be able to visualize it, the nature of the method (applying a rotation and then dropping directions) limits its usefulness. There is a class of algorithms for visualization called manifold learning algorithms that allow for much more complex mappings and often provide better visualizations. A particularly useful one is the t-SNE algorithm. A manifold is a topological space that locally resembles Euclidean space near each point. More precisely, each point of an n-dimensional manifold has a neighbourhood that is homeomorphic to the Euclidean space of dimension n. Manifold learning algorithms are mainly aimed at visualization, and so are rarely\n",
    "used to generate more than two new features.\n",
    "\n",
    "### t-distributed stochastic neighbor embedding\n",
    "\n",
    "The idea behind t-SNE is to find a two-dimensional representation of the data that preserves\n",
    "the distances between points as best as possible. t-SNE starts with a random twodimensional\n",
    "representation for each data point, and then tries to make points that are close in the original feature space closer, and points that are far apart in the original feature space farther apart.\n",
    "\n",
    "\n",
    "`from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=42)\n",
    "digits_tsne = tsne.fit_transform(digits.data)`\n",
    "\n",
    "\n",
    "t-SNE transform does not work well for prediction. It is solely useful as a visualization method (maybe a future modification to the algorithm lets t-SNE transform allow for new points). A general rule of thumb is that larger perplexity works better for large datasets and lower perplexity works well for smaller data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/pca-digits.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/tsne-digits.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Anlysis\n",
    "\n",
    "Discriminant Analysis is actually a classification method but it has succesfully been used as a dimensionality reduction method. It is quite simple due to its generative probabilistic assumptions.\n",
    "\n",
    "$\\large{P(y=k | X) = \\frac{P(X | y=k) P(y=k)}{P(X)} = \\frac{P(X | y=k) P(y = k)}{ \\sum_{l} P(X | y=l) \\cdot P(y=l)}}$\n",
    "\n",
    "$\\large{p(X | y=k) = \\frac{1}{(2\\pi)^n |\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2} (X-\\mu_k)^t \\Sigma^{-1} (X-\\mu_k)\\right)}$\n",
    "\n",
    "From the first equation above, we can see that it is just Bayes rule combined with an assumption regarding the underlying distribution of the data. In Linear Discriminant Analysis, we assume a gaussian distribution. Note the covariance matrix $\\Sigma^{-1}$ in the second equation. \n",
    "\n",
    "\n",
    "### PCA vs Linear Discriminant Analysis\n",
    "\n",
    "- LDA is a supervised alternative to PCA.\n",
    "- Both fit Gaussian model\n",
    "- PCA for the whole data\n",
    "- LDA multiple Gaussians with shared covariance\n",
    "- Can use LDA to transform space!\n",
    "- At most as many components as there are classes (needs between class variance)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is more often used for exploratory analysis and visualization rather than directly in a classifier. \n",
    "\n",
    "### Clustering\n",
    "**&#9755; k-Means Clustering:** Simple, iterative algorithm: pick k random points as cluster centers. While cluster centers change (more than some epsilon), assign each data point to its closest cluster center, then recompute cluster centers as the mean of the assigned points. We often don't know k. How do we go about choosing it? Pick a few different k's at random. k-Means finds a *local* minimum of the following cost function.\n",
    "\n",
    "$$ \\min_{\\mathbf{c}_j \\in \\mathbf{R}^p, j=1,..,k} \\sum_i ||\\mathbf{x}_i - \\mathbf{c}_{x_i}||^2$$\n",
    "\n",
    "k-Means can also be used for feature extraction. We can add a categorical feature denoting cluster membership and/or a continuous feature denoting distance from cluster.(Note, k-Means default in scikit learn runs k-means 10 times for different initializations and returns best one)\n",
    "\n",
    "- scaling greatly affects k-means result\n",
    "- Since k-Means returns a local minimum, it is very sensitive to initialization. \n",
    "- Cluster areas are convex sets, limiting the complexity of the boundaries. \n",
    "- Does not address cluster sizes. \n",
    "\n",
    "\n",
    "**&#9755; Agglomerative Clustering:** refers to a collection of clustering algorithms that build upon the same principles: the algorithm starts by declaring each point its own cluster,\n",
    "and then greedily merges the two most similar clusters until some stopping criterion is satisfied. Three merging (linkage) criterions are implemented in scikit-learn: ward, average, and complete. Ward returns relatively equally sized clusters. We can use scipy to plot dendograms to visualize the development of the agglomerative clustering algorithm. Agglomerative clustering cannot transform new points.\n",
    "\n",
    "\n",
    "**&#9755; DBSCAN:** Density based spatial clustering of applications with noise. DBSCAN does not need the number of clusters to be set a piori, which can be advantageous in some applications. This method however, is very sensitive to its two parameters (min_samples, epsilon). DBSCAN often results in highly imbalanced cluster sizes. DBSCAN IS TRASH IF YOU DON'T HAVE CLASS LABELS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklean.cluster import KMeans\n",
    "km = KMeans(n_clusters=5, random_state=0)\n",
    "km.fit(X)\n",
    "km.transform(X_test)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=km.labels_)  #2d data\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X)\n",
    "ax = plt.subplot()\n",
    "ax.scatter(X[:, 0], X[:, 1], assignment) #2d data\n",
    "\n",
    "# dendogram visualization \n",
    "linkage_array = linkage(X, 'ward')\n",
    "dendrogram(linkage_array, p=30, truncate_mode='level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Evaluation\n",
    "\n",
    "**&#9755; NMI:** Normalized Mutual Information uses the information theory metric of mutual information to measure clustering results to grounds truths. \n",
    "\n",
    "**&#9755; ARI:** Adjusted Rand Index evaluates clusterings by comparing clusters to groundd truths.   \n",
    "\n",
    "**&#9755; Silhouette Score:** An unsupervised cluster evaluation score that favors compact cluster and therefore favors methods like k-Means over DBSCAN. \n",
    "\n",
    "**&#9755; Silhouette Score:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF\n",
    "\n",
    "Non-negative Matrix Factorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection\n",
    "\n",
    "\n",
    "Useful for fraud detection, network failure, intrusion detection in networks etc. \n",
    "\n",
    "Outlier Detection: data set is dirty, data contains outliers\n",
    "\n",
    "Novelty detection: data will be clean. You will be given a new data set and asked to find new points. \n",
    "\n",
    "in both you want to identify 'different' data.\n",
    "\n",
    "Idea, model data distribution p(X). If point is unlikely under model, then it is classified as an outlier. x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Choosing data representation\n",
    "\n",
    "This is among the harder tasks in machine learning. There are a few methods used to verify our representation of the data is 'good'. The simplest verification method is measuring the relative improvement between the data representations using 1-NN classifier. Since 1-NN only relies on the representation of the data, if some 1-NN performs better on one representation, then it indicates our representation of data is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
