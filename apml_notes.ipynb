{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Many models, K-NN and linear models particularly, benefit substantially from scaling (notable exceptions are NN, and Tree based models). We cover a few methods readily available in Scikit-learn and other packages. (i denotes row, j column)  \n",
    "\n",
    "`from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, MaxAbsScaler`  \n",
    "\n",
    "**&#9755; StandardScaler:**  \n",
    "\n",
    "$\\large{x_{ij}' = \\frac{x_{ij} - \\mu_j}{\\sigma_j}} \\quad \\forall i,j$\n",
    "\n",
    "all scaled features now have zero mean and std of 1. Note that scaled data is not bounded since it is only scaled by the\n",
    "standard deviation. An outlier before the transformation will still be an outlier after transformation. StandardScaler is sensitive to outliers as they skew the mean and standard deviation. \n",
    "\n",
    "\n",
    "**&#9755; MinMaxScaler**  \n",
    "\n",
    "$\\large{x_{i,j}' = \\frac{x_{ij} - min(x_j)}{max(x_j) - min(x_j)}}$  \n",
    "\n",
    "maps the minimum of each column to 0 and max to 1. It is easy to see in the transformation that MinMaxScaler is very sensitive to outliers  since it will cluster points in one area of the positive unit quadrant. Useful when data has clearly defined boundaries i.e. greyscale image.  \n",
    "\n",
    "\n",
    "**&#9755; MaxAbsScaler**  \n",
    "Similar to MinMaxScaler except min and max are measured in absolute value. Useful for sparse data. \n",
    "\n",
    "**&#9755; RobustScaler**  \n",
    "Some robust statistics stuff (read interquartile range (IQR) and the median absolute deviation (MAD)). Similar to\n",
    "StandardScaler, except robust to outliers.  \n",
    "\n",
    "**&#9755; Normalizer**  \n",
    "Projects unto the L1 or L0 unit ball (i.e. makes sure vectors have length 1 either in euclidean measure or L1 measure). Can't think of use cases atm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls\n",
    "\n",
    "**&#9841; Scaling Sparse data:**  \n",
    "Do not center sparse data (i.e. apply zero mean, unit variance or MinMaxScaler) since this will make the matrix not sparse anymore and blow up RAM and CPU. Scale by a constant factor since constant times zero is zero, preserving sparsity. Use MaxAbsScaler for sparse data.  \n",
    "\n",
    "**&#9841; Including test set in scaler's `fit()`**  \n",
    "Including the test set in `fit()` will lead to artificially higher accuracy scores since we are parameterizing our scaler based on our test set. In deployment, we obviously can't parameterize our scalers on unseen data. \n",
    "\n",
    "**&#9841; Calling `fit()` on training and test set separately:**  \n",
    "Never call `fit()` on the test set separately since it will change the relationship of test points to training points. Only call fit on training set. Then call transform on both training and test sets. \n",
    "\n",
    "**&#9841; Not using pipelines in cross validation:**  \n",
    "When you perform cross validation on the scaled training set, the validation fold is scaled in the same way as the training fold, leading to pitfall 2. Using pipelines solves this pitfall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"files/images/scaling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# this is wrapper for Pipeline below\n",
    "pipe = make_pipeline(StandardScaler(), Ridge())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)\n",
    "\n",
    "# this gives flexibility in naming the steps. Useful when tuning parameters.\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline(((\"scaler\", StandardScaler()),\n",
    "                 (\"regressor\", KNeighborsRegressor)))\n",
    "\n",
    "# cross validation with pipeline\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "scores = cross_val_score(knn_pipe, X_train, y_train, cv=10)\n",
    "np.mean(scores), np.std(scores)\n",
    "\n",
    "\n",
    "# pipeline and GridSearchCV. Use modified names ([step_name]__[parameter]) \n",
    "# in GridSearch param_grid\n",
    "knn_pipe = make_pipeline(('scale', StandardScaler()), ('model', KNeighborsRegressor())\n",
    "param_grid = {'model__n_neighbors': range(1, 10)}\n",
    "grid = GridSearchCV(knn_pipe, param_grid, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Distributions \n",
    "\n",
    "Linear Models may perform better when features are normally distributed. Scaling data does not change the distribution of the points. There are several transformations available that do just this. Most common tranformations are power transformations, particularly the Box Cox Transformation.  \n",
    "\n",
    "**&#9755; Box-Cox transformation:**  \n",
    "\n",
    "$bc_{\\lambda}(x) = \\cases{\\frac{x^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0\\cr log(x) & \\text{if } \\lambda = 0 }$  \n",
    "\n",
    "only applicable for positive x!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearn 0.20-dev\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer(method='box-cox') \n",
    "# soon: method='Yeo-Johnson'\n",
    "pt.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data  \n",
    "\n",
    "We can one-hot encode a feature's categorical values (e.g. $\\{'green', 'blue', 'yellow'\\}$) using pandas. We can encode all k values, introducing redundancies, or k-1 to have more accurate interpretation of coefficients. We can also one hot encode using sklearn but this method requires categorical values to be in integer format.  \n",
    "  \n",
    "We face a problem when categorical feature values are of high cardiniality (50, 100, etc). We must find a way to compress the values into fewer than k features. This solution is often specific to the dataset. \n",
    "\n",
    "## Pitfalls\n",
    "\n",
    "**&#9841; Categorical Values in test but not training set:**  \n",
    "Make sure to one hot encode all possible values of the feature, not just those in the test and traning sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'salary': [103, 89, 142, 54, 63, 219],\n",
    "                   'boro': ['Manhattan', 'Queens', 'Manhattan',\n",
    "                            'Brooklyn', 'Brooklyn', 'Bronx']})\n",
    "\n",
    "# note that there are more categories in the definition of the \n",
    "# column than those seen in the column\n",
    "df['boro'] = pd.Categorical(\n",
    "    df.boro, categories=['Manhattan', 'Queens', 'Brooklyn', 'Bronx', 'Staten Island'])\n",
    "pd.get_dummies(df, columns=['boro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature interactions  \n",
    "\n",
    "Linear models particularly benefit from feature interactions. Feature transformations allow models to fit non linear boundaries or curves to the data. However, these transformations blow up the feature space. Using kernel transformations allow for the power of the transformations without a significant increase in CPU or memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation  \n",
    "\n",
    "Imputation refers to dealing with missing values. We can always drop columns with NaN values but we would lose all the information given by non NaN values. We could also drop the observations that contain NaN values if they are few. There are 4 types of imputation methods that try to extrapolate missing values: Mean/median, kNN, regression, probabilistic. A general good practice with imputation is to create a dummy indicating if the value was NaN in addition to extrapolation of the value. Check out fancyimpute library (note that fancy impute does not implement fit/transform paradigm, thus does not work with pipelines...information leak!)\n",
    "\n",
    "**&#9755; Mean Imputation:**  \n",
    "Mean imputation fills in NaN values with the mean of the given feature. This, of course, only works for non binary data. Mean imputation can be acceptable if missing values are few. Else, this method can destroy the data distribution, hiding useful structural relationships between the input and target data.  \n",
    "\n",
    "**&#9755; kNN Imputation:**  \n",
    "kNN imputation works by taking k nearest neighbors of the observation with the missing value, and replaces NaN with the average value of that feature among the k neighbors. Note that kNN imputation only works if the features used to compute distance are not NaN, since Euclidiean distance is undefined for NaN values. So points with NaN values are thown away when computing  k neighbors.  \n",
    "\n",
    "**&#9755; Model Driven Imputation:**  \n",
    "Train a model on the non missing features to predict missing features (kNN imputation is arguably Model driven imputation but there are some slight differences). A popular model to use is random forests (see code below). \n",
    "\n",
    "**&#9755; (MICE) Multiple Imputation by Chained Equations:**  \n",
    "Not sure yet how it works but this is a very popular imputation method. Not in sklearn, but is in fancyimpute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-dc41b017f0c6>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-dc41b017f0c6>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    <img src=\"files/images/imputation_methods.png\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# mean imputation\n",
    "from sklean.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(strategy='mean').fit(X_train)\n",
    "imp.transform(X_train)\n",
    "\n",
    "\n",
    "# kNN imputation: very inefficient didactic implementation\n",
    "# use library!\n",
    "distances = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "for i, x1 in enumerate(X_train):\n",
    "    for j, x2 in enumerate(X_train):\n",
    "        dist = (x1 - x2) ** 2\n",
    "        nan_mask = np.isnan(dist)\n",
    "        distances[i, j] = dist[~nan_mask].mean() * X_train.shape[1]\n",
    "neighbors = np.argsort(distances, axis=1)[:, 1:]\n",
    "n_neighbors = 3\n",
    "X_train_knn = X_train.copy()\n",
    "for feature in range(X_train.shape[1]):\n",
    "    has_missing_value = np.isnan(X_train[:, feature])\n",
    "    for row in np.where(has_missing_value)[0]:\n",
    "        neighbor_features = X_train[neighbors[row], feature]\n",
    "        non_nan_neighbors = neighbor_features[~np.isnan(neighbor_features)]\n",
    "        X_train_knn[row, feature] = non_nan_neighbors[:n_neighbors].mean()\n",
    "    \n",
    "    \n",
    "# Model Driven Imputation with random forests.     \n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "X_imputed = X_train.copy()\n",
    "for i in range(10):\n",
    "    last = X_imputed.copy()\n",
    "    for feature in range(X_train.shape[1]):\n",
    "        inds_not_f = np.arange(X_train.shape[1])\n",
    "        inds_not_f = inds_not_f[inds_not_f != feature]\n",
    "        f_missing = np.isnan(X_train[:, feature])\n",
    "        rf.fit(X_imputed[~f_missing][:, inds_not_f], X_train[~f_missing, feature])\n",
    "        X_imputed[f_missing, feature] = rf.predict(\n",
    "            X_imputed[f_missing][:, inds_not_f])\n",
    "    if (np.linalg.norm(last - X_imputed)) < .5:\n",
    "        break\n",
    "scores = cross_val_score(logreg, X_imputed, y_train, cv=10)\n",
    "np.mean(scores)\n",
    "\n",
    "\n",
    "# MICE with fancyimpute\n",
    "import fancyimpute\n",
    "\n",
    "mice = fancyimpute.MICE(verbose=0)\n",
    "X_train_fancy_mice = mice.complete(X_train)\n",
    "scores = cross_val_score(logreg, X_train_fancy_mice, y_train, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/imputation_methods.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection  \n",
    "\n",
    "It is often beneficial to reduce the feature space. It leads to faster predictions and faster traning times. Less data to gather when model is in production. Less storage for model and dataset and more importantly, leads to more interpretable models. There are supervised and unsupervised feature selection methods.  \n",
    "\n",
    "**&#9755; Covariance based:**  \n",
    "When two features are highly correlated, we can remove 1 of them without substantially affecting model predictions (probably..it could be the small difference in variation of these features was really important for predictions. We can test this easily though.) Show code for sorting correlation matrix heatmap.\n",
    "\n",
    "**&#9755; PCA:**  \n",
    "PCA maps the data to a linear subpsace. Only reduces feature space for training and predictions, not for data gathering. It makes model less interpretable BUT it can lead to very useful visualizations in 2D and 3D and it can speed up training time considerably. It can remove useful information. \n",
    "\n",
    "**&#9755; Univariate Statistics:**  \n",
    "We can build a linear regression model on single feature and the target and measure F and p values for the coefficient. These values let us know if feature is important for prediction. However, linear regression assumes linear relationship between input and target, which may be a poor assumption. Furthermore, the linear assumption decreases the importance of binary features.  \n",
    "\n",
    "Mutual information relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances. MI Can deal with binary features (must specify which columns are binary). This is a good choice when assuming non linear interaction between feature and target. However, much more computationally intensive than f regression.  \n",
    "\n",
    "**&#9755; Model Based feature selection:**  \n",
    "Train a model and check coefficients (in the case of linear models) or splits (in the case of tree based models). Discard features with coefficient close to zero or without/non informative splits. This is usually a good choice but computationally expensive. \n",
    "\n",
    "**&#9755; Iterative Model based feature selection:**  \n",
    "Removing many features at once may change the model more than we indend to. A better way to iteratively remove a single feature, each time retraining the model and selecting the least important. We can do this with RFE (Recursive Feature Elimination) in Sklearn. We can also iterate the opposite way, Sequential Feature Selection from library mlextend. \n",
    "\n",
    "\n",
    "## Pitfalls  \n",
    "\n",
    "**&#9841; Univariate statistics do not account for correlation:**  \n",
    "Univariate statistics give highly correlated features the same importance. Model driven feature selection accounts for correlation and can thus give much higher importance to some features while removing importance from others (which is which is usually random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get F and p values so we can visualize and drop features\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "f_values, p_values = f_regression(X, y)\n",
    "\n",
    "\n",
    "# or we can use SelectKBest to automatically drop features below a threshold. \n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFpr\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "\n",
    "select = SelectKBest(k=2, score_func=f_regression)\n",
    "select.fit(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(select.transform(X_train).shape)\n",
    "\n",
    "# SelectKBest with pipelines\n",
    "make_pipeline(StandardScaler(), SelectKBest(k=2, score_func=f_regression), RidgeCV())\n",
    "\n",
    "# Mutual information\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "scores = mutual_info_regression(X_train, y_train, discrete_features=[3])\n",
    "\n",
    "# Model Driven feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "select_lassocv = SelectFromModel(LassoCV(), threshold=1e-5)\n",
    "select_lassocv.fit(X_train, y_train)\n",
    "print(select_lassocv.transform(X_train).shape)\n",
    "\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# create ranking among all features by selecting only one\n",
    "rfe = RFE(LinearRegression(), n_features_to_select=1)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "rfe.ranking_\n",
    "\n",
    "# RFE that automatically selects best number of features to keep\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfe = RFECV(LinearRegression(), cv=10)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "print(rfe.support_)\n",
    "print(boston.feature_names[rfe.support_])\n",
    "\n",
    "# Sequential Feature Selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "sfs = SequentialFeatureSelector(LinearRegression(), forward=False, k_features=7)\n",
    "sfs.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dimensionality Reduction  \n",
    "\n",
    "Transforming data using unsupervised learning can have many motivations. The most common motivations are visualization, compressing the data, and finding a representation that is more informative for further processing.\n",
    "\n",
    "## PCA\n",
    "\n",
    "On a high level, principal component analysis iteratively does the following: \n",
    "\n",
    "Finds the direction, orthogonal to all previously established directions, along which the data points vary the most (highest variance). Describe this direction with a unit vector. Continue until we have n directions. Build an n-dimensional linear map with these n vectors.\n",
    "\n",
    "These n vectors, representing n directions are called principal components. For n dimensional data, an n-dimensional PCA is just a rotation (re-basing of the vector space) in which the dimensions are sorted in order of decreasing variance. \n",
    "\n",
    "TODO talk about PCA equation  \n",
    "\n",
    "TODO Whitening\n",
    "\n",
    "We usually scale the the data to have unit standard deviationg before applying PCA (PCA normalizes to 0 mean under the covers). It is important to note that PCA is an unsupervised method, and does not use any class\n",
    "information when finding the rotation. It simply looks at the correlations in the data.  \n",
    "\n",
    "## Pitfalls  \n",
    "\n",
    "**&#9841; Uneven class distributions**  \n",
    "When our dataset has skewed distributions (or outliers), PCA will give a lot of weight to the classes with many observations. We need to randomly throw out observations of commonly occuring classes so most classes have similar occurence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"files/images/pca-intuition.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_train_scaled = StandardScaler(X_train)\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
